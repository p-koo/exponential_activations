{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(33) # for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get ground truth motifs from JASPAR database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jaspar_motifs(file_path):\n",
    "    def get_motif(f):\n",
    "        line = f.readline()\n",
    "        name = line.strip().split()[1]\n",
    "        pfm = []\n",
    "        for i in range(4):\n",
    "            line = f.readline()\n",
    "            if len(line.split()[1]) > 1:\n",
    "                pfm.append(np.asarray(np.hstack([line.split()[1][1:], line.split()[2:-1]]), dtype=float))\n",
    "            else:\n",
    "                pfm.append(np.asarray(line.split()[2:-1], dtype=float))\n",
    "        pfm = np.vstack(pfm)\n",
    "        sum_pfm = np.sum(pfm, axis=0)\n",
    "        pwm = pfm/np.outer(np.ones(4), sum_pfm)\n",
    "        line = f.readline()\n",
    "        return name, pwm\n",
    "\n",
    "    num_lines = sum(1 for line in open(file_path))\n",
    "    num_motifs = int(num_lines/6)\n",
    "\n",
    "    f = open(file_path)\n",
    "    tf_names = []\n",
    "    tf_motifs = []\n",
    "    for i in range(num_motifs):\n",
    "        name, pwm = get_motif(f)\n",
    "        tf_names.append(name)\n",
    "        tf_motifs.append(pwm)\n",
    "\n",
    "    return tf_motifs, tf_names\n",
    "\n",
    "# parse JASPAR motifs\n",
    "savepath = '../../data'\n",
    "file_path = os.path.join(savepath, 'pfm_vertebrates.txt')\n",
    "motif_set, motif_names = get_jaspar_motifs(file_path)\n",
    "\n",
    "# get a subset of core motifs \n",
    "core_names = ['Arid3a', 'CEBPB', 'FOSL1', 'Gabpa', 'MAFK', 'MAX', \n",
    "              'MEF2A', 'NFYB', 'SP1', 'SRF', 'STAT1', 'YY1']\n",
    "\n",
    "strand_motifs = []\n",
    "core_index = []\n",
    "for name in core_names:\n",
    "    strand_motifs.append(motif_set[motif_names.index(name)])\n",
    "    core_index.append(motif_names.index(name))\n",
    "\n",
    "# generate reverse compliments\n",
    "core_motifs = []\n",
    "for pwm in strand_motifs:\n",
    "    core_motifs.append(pwm)\n",
    "    reverse = pwm[:,::-1]\n",
    "    core_motifs.append(reverse[::-1,:]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions to generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(core_motifs, seq_length):\n",
    "    \n",
    "    num_motif = len(core_motifs)\n",
    "    cum_dist = np.cumsum([0, 0.5, 0.25, 0.17, 0.05, 0.3])\n",
    "    \n",
    "    # sample core motifs for each grammar\n",
    "    valid_sim = False\n",
    "    while not valid_sim:\n",
    "\n",
    "        # determine number of core motifs in a given grammar model\n",
    "        num_interactions = np.where(np.random.rand() > cum_dist)[0][-1]+1 #np.random.randint(min_interactions, max_interactions)\n",
    "\n",
    "        # randomly sample motifs\n",
    "        sim_motifs = np.random.randint(num_motif, size=num_interactions)\n",
    "        num_sim_motifs = len(sim_motifs)\n",
    "        #sim_motifs = sim_motifs[np.random.permutation(num_sim_motifs)]\n",
    "        \n",
    "        # verify that distances aresmaller than sequence length\n",
    "        distance = 0\n",
    "        for i in range(num_sim_motifs):\n",
    "            distance += core_motifs[sim_motifs[i]].shape[1]\n",
    "        if seq_length > distance > 0:\n",
    "            valid_sim = True    \n",
    "\n",
    "    # simulate distances between motifs + start \n",
    "    valid_dist = False\n",
    "    while not valid_dist:\n",
    "        remainder = seq_length - distance\n",
    "        sep = np.random.uniform(0, 1, size=num_sim_motifs+1)\n",
    "        sep = np.round(sep/sum(sep)*remainder).astype(int)\n",
    "        if np.sum(sep) == remainder:\n",
    "            valid_dist = True\n",
    "\n",
    "    # build a PWM for each regulatory grammar\n",
    "    pwm = np.ones((4,sep[0]))/4\n",
    "    for i in range(num_sim_motifs):\n",
    "        pwm = np.hstack([pwm, core_motifs[sim_motifs[i]], np.ones((4,sep[i+1]))/4])\n",
    "\n",
    "    return pwm, sim_motifs\n",
    "\n",
    "\n",
    "def simulate_sequence(sequence_pwm):\n",
    "    \"\"\"simulate a sequence given a sequence model\"\"\"\n",
    "\n",
    "    nucleotide = 'ACGT'\n",
    "\n",
    "    # sequence length\n",
    "    seq_length = sequence_pwm.shape[1]\n",
    "\n",
    "    # generate uniform random number for each nucleotide in sequence\n",
    "    Z = np.random.uniform(0,1,seq_length)\n",
    "\n",
    "    # calculate cumulative sum of the probabilities\n",
    "    cum_prob = sequence_pwm.cumsum(axis=0)\n",
    "\n",
    "    # go through sequence and find bin where random number falls in cumulative \n",
    "    # probabilities for each nucleotide\n",
    "    one_hot_seq = np.zeros((4, seq_length))\n",
    "    for i in range(seq_length):\n",
    "        index=[j for j in range(4) if Z[i] < cum_prob[j,i]][0]\n",
    "        one_hot_seq[index,i] = 1\n",
    "    return one_hot_seq\n",
    "\n",
    "\n",
    "def get_label(labels, max_labels):\n",
    "    unique_labels = np.unique(np.floor(labels/2).astype(int))\n",
    "    \n",
    "    targets = np.zeros((1, max_labels))\n",
    "    \n",
    "    for i in unique_labels:\n",
    "        targets[0, i] = 1\n",
    "    \n",
    "    return targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate synthetic sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset parameters\n",
    "num_seq = 30000             # number of sequences\n",
    "seq_length = 200            # lsaength of sequence\n",
    "min_interactions = 1        # exponential rate of number of motifs for each grammar\n",
    "max_interactions = 5\n",
    "max_labels = len(core_names)\n",
    "\n",
    "# generate sythetic sequences as a one-hot representation\n",
    "seq_pwm = []\n",
    "seq_model = []    \n",
    "targets = []\n",
    "for j in range(num_seq):\n",
    "    signal_pwm, labels = generate_model(core_motifs, seq_length)\n",
    "    seq_pwm.append(simulate_sequence(signal_pwm))\n",
    "    targets.append(get_label(labels, max_labels))\n",
    "    seq_model.append(signal_pwm)\n",
    "    \n",
    "targets = np.vstack(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset into train, validation, and test sets and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataset into train, cross-validation, and test\n",
      "Generating training data\n",
      "Generating cross-validation data\n",
      "Generating test data\n",
      "Saving to: ../data/synthetic_dataset.h5\n"
     ]
    }
   ],
   "source": [
    "def split_data(data, label, model, split_size):\n",
    "    \"\"\"split data into train set, cross-validation set, and test set\"\"\"\n",
    "    \n",
    "    def subset_data(data, label, model, sub_index):\n",
    "        \"\"\"returns a subset of the data and labels based on sub_index\"\"\"\n",
    "        \n",
    "        num_sub = len(sub_index)\n",
    "        sub_set_label = []\n",
    "        sub_set_seq = []\n",
    "        sub_set_model = []\n",
    "        for index in sub_index:\n",
    "            sub_set_seq.append([data[index]])\n",
    "            sub_set_label.append(label[index])\n",
    "            sub_set_model.append([model[index]])\n",
    "        sub_set_seq = np.vstack(sub_set_seq)\n",
    "        sub_set_label = np.vstack(sub_set_label)\n",
    "        sub_set_model = np.vstack(sub_set_model)\n",
    "    \n",
    "        return (sub_set_seq, sub_set_label, sub_set_model)\n",
    "\n",
    "    \n",
    "    # determine indices of each dataset\n",
    "    N = len(data)\n",
    "    cum_index = np.cumsum(np.multiply([0, split_size[0], split_size[1], split_size[2]],N)).astype(int) \n",
    "\n",
    "    # shuffle data\n",
    "    shuffle = np.random.permutation(N)\n",
    "\n",
    "    # training dataset\n",
    "    train_index = shuffle[range(cum_index[0], cum_index[1])]\n",
    "    cross_validation_index = shuffle[range(cum_index[1], cum_index[2])]\n",
    "    test_index = shuffle[range(cum_index[2], cum_index[3])]\n",
    "\n",
    "    # create subsets of data based on indices \n",
    "    print('Generating training data')\n",
    "    train = subset_data(data, label, model, train_index)\n",
    "\n",
    "    print('Generating cross-validation data')\n",
    "    cross_validation = subset_data(data, label, model, cross_validation_index)\n",
    "    \n",
    "    print('Generating test data')    \n",
    "    test = subset_data(data, label, model, test_index)\n",
    "    \n",
    "    return train, cross_validation, test\n",
    "\n",
    "\n",
    "def save_dataset(savepath, train, valid, test):\n",
    "    f = h5py.File(savepath, \"w\")\n",
    "    dset = f.create_dataset(\"X_train\", data=train[0], compression=\"gzip\")\n",
    "    dset = f.create_dataset(\"Y_train\", data=train[1], compression=\"gzip\")\n",
    "    dset = f.create_dataset(\"model_train\", data=train[2], compression=\"gzip\")\n",
    "    dset = f.create_dataset(\"X_valid\", data=valid[0], compression=\"gzip\")\n",
    "    dset = f.create_dataset(\"Y_valid\", data=valid[1], compression=\"gzip\")\n",
    "    dset = f.create_dataset(\"model_valid\", data=valid[2], compression=\"gzip\")\n",
    "    dset = f.create_dataset(\"X_test\", data=test[0], compression=\"gzip\")\n",
    "    dset = f.create_dataset(\"Y_test\", data=test[1], compression=\"gzip\")\n",
    "    dset = f.create_dataset(\"model_test\", data=test[2], compression=\"gzip\")\n",
    "    f.close()\n",
    "    \n",
    "    \n",
    "# split into training, cross-validation, and test sets\n",
    "print(\"Splitting dataset into train, cross-validation, and test\")\n",
    "train_size = 0.7\n",
    "cross_validation_size = 0.1\n",
    "test_size = 0.20\n",
    "split_size = [train_size, cross_validation_size, test_size]\n",
    "train, valid, test = split_data(seq_pwm, targets, seq_model, split_size)\n",
    "\n",
    "# save to file\n",
    "filename =  'synthetic_dataset.h5' \n",
    "file_path = os.path.join(savepath, filename)\n",
    "print('Saving to: ' + file_path)\n",
    "save_dataset(file_path, train, valid, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0b3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
